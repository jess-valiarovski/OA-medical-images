# ğŸ¥ **Descriptive Analysis of User Data from a Medical Image Labeling Mobile Application**  

## ğŸš€ **Project Title:**  
**Evaluating Crowd vs. Expert Labeling for Medical Image Analysis**  

ğŸ“… **Timeframe:** *Feburary 2023-April 2023*  
ğŸ“ **Author:** **Jess Valiarovski**  
ğŸ“š **Company:** **Centaur Labs**  
ğŸ“ **Acknowledgements:** Thank you **Anant Jain** for your mentorship and **Open Avenues** for the internship opprotunity!  


## ğŸš€ **Project Overview**  
This project examines **the reliability of crowdsourced medical image labeling** compared to expert annotations. The goal is to **justify reducing the number of experts needed** to create labeled medical image datasets for **artificial intelligence training**, while maintaining high accuracy.  

### ğŸ” **Key Objectives:**  
- **Compare labeling accuracy** between crowdsourced users and medical experts.  
- **Analyze agreement rates** within and between groups.  
- **Identify whether crowdsourced annotations** can match expert consensus.  
- **Propose cost-saving strategies** by reducing the required number of experts.  

---
### **ğŸ“Š Dataset Overview**  
- **Source:** Medical image labeling mobile application  
- **Task:** Users labeled medical images as **normal** or **abnormal**  
- **User Categories:**  
  - ğŸ¥ **Medical Experts:** Verified professionals with domain expertise  
  - ğŸŒ **Crowd Workers:** General users with no medical background  
---

## ğŸ› ï¸ **2. Methods & Data Processing**  

### ğŸ“Š **Data Cleaning & Transformation**  
- **Pandas (`pandas`):** Data wrangling & preprocessing  
- **NumPy (`numpy`):** Statistical calculations  
- **Regular Expressions (`re`):** Extracting expert votes from URLs  
- **Handling Missing Data:** Filtering out incomplete or irrelevant rows  

### ğŸ”¬ **Data Preparation**  
âœ” **Removed irrelevant columns** (e.g., content metadata, unused votes).  
âœ” **Filtered NA values** and standardized labels.  
âœ” **Extracted expert vote counts** from dataset metadata.  
âœ” **Created new variables** for agreement scores and error rates.  

### ğŸ“Š **Statistical Methods Applied**  
âœ” **Agreement Analysis:** Calculated expert consensus and crowd consensus.  
âœ” **Error Rate Calculation:** Measured expert/crowd misclassification rates.  
âœ” **Confidence Categories:** Labeled cases based on labeling difficulty.  

### ğŸ“ˆ **Data Visualization (`plotly`, `matplotlib`)**  
- **Density Heatmaps** â€“ Show relationship between group size & labeling accuracy  
- **Violin Plots** â€“ Compare expert and crowd vote distributions  
- **Agreement Distributions** â€“ Show variation in normal vs. abnormal classifications  
---
## ğŸ›   **Key Findings & Visuals**  
- **The crowd was overall more confident** while **experts were more frequently split** in their diagnostic labeling.
- **Only 0.92% of cases reached unanimous expert agreement**, while **5.77% of cases reached unanimous crowd agreement**.
- 
### ğŸ”¬ **Conclusion**  
ğŸ“Œ **Aggregated crowd opinions were more consistent** with final expert consensus than individual expert votes.
ğŸ“Œ **Crowdsourced medical image labeling can reduce reliance on medical experts and cut sourcing costs** and 
also **generate high-quality labeled medical imaging datasets for AI diagnostic training**.  

---

## ğŸ“Œ **Technical Skills Demonstrated**  

ğŸ›  **Python & Data Science Techniques**  
âœ… **Data Wrangling & Cleaning:** `pandas`, `numpy`, `regex`  
âœ… **Statistical Modeling & Analysis:** Agreement rates, difficulty scores, expert error rates  
âœ… **Data Visualization:** `plotly`, `matplotlib`, `plotly.express`  
âœ… **Hypothesis Testing:** Consensus comparison between experts and crowd  

---

## ğŸ“¢ **How to View the Analysis**  

ğŸ“Œ **Jupyter Notebook:**  
ğŸ“ [exploratory_analysis-final-4.ipynb](./exploratory_analysis-final-4.ipynb)  

---
